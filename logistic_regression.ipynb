{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:  What is Logistic Regression, and how does it differ from Linear\n",
        "Regression?\n",
        "\n",
        "**Logistic Regression:**\n",
        "\n",
        "Logistic Regression is a statistical method used for **binary classification problems** — that is, when the output variable is categorical with two possible outcomes (e.g., yes/no, 0/1, true/false). It estimates the probability that a given input belongs to a particular class.\n",
        "\n",
        "* **Model:** It models the probability that the dependent variable $Y$ belongs to a particular category using the **logistic function (sigmoid function)**:\n",
        "\n",
        "  $$\n",
        "  P(Y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n)}}\n",
        "  $$\n",
        "\n",
        "* **Output:** The output is a probability value between 0 and 1, which can then be thresholded (usually at 0.5) to classify the input into one of the two classes.\n",
        "\n",
        "* **Purpose:** Used for classification tasks.\n",
        "\n",
        "---\n",
        "\n",
        "**Linear Regression:**\n",
        "\n",
        "Linear Regression is used for **predicting a continuous dependent variable** based on one or more independent variables.\n",
        "\n",
        "* **Model:** It models the relationship between the dependent variable $Y$ and independent variables $X$ as a linear equation:\n",
        "\n",
        "  $$\n",
        "  Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n + \\epsilon\n",
        "  $$\n",
        "\n",
        "* **Output:** The output is a continuous numerical value.\n",
        "\n",
        "* **Purpose:** Used for regression (predicting continuous outcomes).\n",
        "\n",
        "---\n",
        "\n",
        "### Key Differences:\n",
        "\n",
        "| Aspect              | Logistic Regression                          | Linear Regression               |\n",
        "| ------------------- | -------------------------------------------- | ------------------------------- |\n",
        "| **Output Variable** | Categorical (binary classification)          | Continuous numerical prediction |\n",
        "| **Model Function**  | Logistic (sigmoid) function                  | Linear function                 |\n",
        "| **Output Range**    | Between 0 and 1 (interpreted as probability) | Any real number                 |\n",
        "| **Loss Function**   | Uses Log Loss (Cross-Entropy)                | Uses Mean Squared Error (MSE)   |\n",
        "| **Purpose**         | Classification                               | Regression                      |\n",
        "| **Interpretation**  | Estimates probability of class membership    | Estimates expected value of $Y$ |\n",
        "\n",
        "---\n",
        "\n",
        "### Summary:\n",
        "\n",
        "* Logistic regression is suitable when the goal is to **classify** data into two classes by modeling probabilities.\n",
        "* Linear regression is used when predicting a **continuous value**.\n",
        "* Logistic regression uses the **sigmoid function** to ensure outputs stay between 0 and 1.\n",
        "* Linear regression produces an unrestricted continuous output.\n",
        "\n",
        "---\n",
        "\n",
        "Question 2: Explain the role of the Sigmoid function in Logistic Regression.\n",
        "\n",
        "\n",
        "### Introduction\n",
        "\n",
        "In Logistic Regression, the goal is to predict the probability that a given input belongs to a particular class — usually in binary classification, where the output variable can take one of two values (0 or 1). To do this effectively, the model needs a way to convert any input (which can be any real number) into a valid probability (a number between 0 and 1). This is where the **Sigmoid function** plays a crucial role.\n",
        "\n",
        "---\n",
        "\n",
        "### Mathematical Definition of the Sigmoid Function\n",
        "\n",
        "The Sigmoid function, also known as the logistic function, is defined as:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $z$ is a linear combination of the input features:\n",
        "\n",
        "$$\n",
        "z = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n\n",
        "$$\n",
        "\n",
        "* $e$ is the base of the natural logarithm (approximately 2.71828).\n",
        "\n",
        "The output of the Sigmoid function, $\\sigma(z)$, always lies between 0 and 1 regardless of the value of $z$.\n",
        "\n",
        "---\n",
        "\n",
        "### Role of the Sigmoid Function in Logistic Regression\n",
        "\n",
        "#### 1. **Converting Linear Output to Probability**\n",
        "\n",
        "* Logistic regression starts by computing a weighted sum of input features (the linear function $z$), which can take any value from $-\\infty$ to $+\\infty$.\n",
        "* This raw output $z$ by itself is **not interpretable as a probability** because probabilities must lie in the range \\[0, 1].\n",
        "* The Sigmoid function **“squashes”** or transforms this linear output into a value between 0 and 1, which is interpretable as the **probability of the positive class**.\n",
        "\n",
        "This can be written as:\n",
        "\n",
        "$$\n",
        "P(Y=1 | X) = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "where $P(Y=1 | X)$ is the predicted probability that the output $Y$ is 1 given input features $X$.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Smooth Non-linear Transformation**\n",
        "\n",
        "* The Sigmoid function is **non-linear** and continuous, producing an S-shaped curve.\n",
        "* It transitions smoothly from values close to 0 when $z$ is very negative, to values close to 1 when $z$ is very positive.\n",
        "* This smoothness allows logistic regression to model probabilities that change gradually with changes in the input features, capturing uncertainty effectively.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Thresholding for Classification**\n",
        "\n",
        "* While logistic regression predicts probabilities, the actual classification into classes 0 or 1 is done by applying a **decision threshold** to the output probability.\n",
        "* The most common threshold is **0.5**:\n",
        "\n",
        "  * If $\\sigma(z) \\geq 0.5$, predict class 1.\n",
        "  * If $\\sigma(z) < 0.5$, predict class 0.\n",
        "\n",
        "This thresholding approach translates the probabilistic output into a **binary decision**.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **Differentiability and Optimization**\n",
        "\n",
        "* A key feature of the Sigmoid function is that it is **differentiable everywhere**, which means it has a well-defined derivative for all input values.\n",
        "* This differentiability is essential for training the logistic regression model using optimization algorithms like **Gradient Descent**.\n",
        "* The derivative of the Sigmoid function is:\n",
        "\n",
        "$$\n",
        "\\sigma'(z) = \\sigma(z) \\times (1 - \\sigma(z))\n",
        "$$\n",
        "\n",
        "* This neat property allows efficient computation of gradients required to update the model parameters $\\beta$.\n",
        "\n",
        "---\n",
        "\n",
        "### Intuition Behind the Sigmoid Function\n",
        "\n",
        "* When $z$ is a large positive number, $e^{-z}$ approaches 0, so:\n",
        "\n",
        "$$\n",
        "\\sigma(z) \\approx \\frac{1}{1 + 0} = 1\n",
        "$$\n",
        "\n",
        "* When $z$ is a large negative number, $e^{-z}$ becomes very large, so:\n",
        "\n",
        "$$\n",
        "\\sigma(z) \\approx \\frac{1}{1 + \\infty} = 0\n",
        "$$\n",
        "\n",
        "* When $z = 0$:\n",
        "\n",
        "$$\n",
        "\\sigma(0) = \\frac{1}{1 + 1} = 0.5\n",
        "$$\n",
        "\n",
        "So, the Sigmoid function smoothly maps any input value to a probability between 0 and 1 with a natural “decision boundary” at 0.5.\n",
        "\n",
        "---\n",
        "\n",
        "### Why Not Use a Linear Function?\n",
        "\n",
        "* If logistic regression used a simple linear function (like linear regression), the output could be any real number — including values less than 0 or greater than 1 — which are **invalid as probabilities**.\n",
        "* Using the Sigmoid function ensures the model’s outputs are **well-calibrated probabilities**, which makes it possible to interpret, analyze, and threshold the results effectively.\n",
        "* This is critical in classification tasks, especially when you want to understand **uncertainty** and **confidence** of predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### Visualization\n",
        "\n",
        "The Sigmoid function looks like this (graphically):\n",
        "\n",
        "* **Input $z$ (horizontal axis):** Ranges from negative to positive values.\n",
        "* **Output $\\sigma(z)$ (vertical axis):** Ranges from 0 to 1.\n",
        "* The curve starts near 0 for large negative inputs, smoothly rises through 0.5 at $z=0$, and asymptotically approaches 1 for large positive inputs.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "\n",
        "The Sigmoid function is the **heart of logistic regression**, performing the critical role of:\n",
        "\n",
        "* **Transforming linear outputs into probabilities** between 0 and 1.\n",
        "* Providing a **smooth and differentiable mapping** suitable for optimization.\n",
        "* Enabling **probabilistic interpretation** of class membership.\n",
        "* Supporting **threshold-based classification decisions**.\n",
        "\n",
        "Without the Sigmoid function, logistic regression would lose its fundamental ability to predict meaningful probabilities and perform effective binary classification.\n",
        "\n",
        "---\n",
        "\n",
        "Question 3: What is Regularization in Logistic Regression and why is it needed?\n",
        "\n",
        "### Introduction\n",
        "\n",
        "**Regularization** is a technique used to prevent **overfitting** in machine learning models, including Logistic Regression. Overfitting happens when a model learns not only the underlying patterns but also the noise in the training data, causing poor generalization to new, unseen data.\n",
        "\n",
        "In logistic regression, regularization helps control the complexity of the model by adding a **penalty** to the loss function based on the size of the model parameters (coefficients). This encourages simpler models that generalize better.\n",
        "\n",
        "---\n",
        "\n",
        "### What is Regularization?\n",
        "\n",
        "Regularization modifies the objective function of logistic regression by adding a penalty term on the magnitude of the model coefficients $\\beta$.\n",
        "\n",
        "---\n",
        "\n",
        "### Objective Function Without Regularization\n",
        "\n",
        "Logistic regression is typically trained by minimizing the **log loss** (also called cross-entropy loss):\n",
        "\n",
        "$$\n",
        "L(\\beta) = - \\frac{1}{m} \\sum_{i=1}^m \\left[ y^{(i)} \\log \\hat{y}^{(i)} + (1 - y^{(i)}) \\log (1 - \\hat{y}^{(i)}) \\right]\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $m$ is the number of training samples,\n",
        "* $y^{(i)}$ is the true label for the $i^{th}$ example,\n",
        "* $\\hat{y}^{(i)} = \\sigma(z^{(i)})$ is the predicted probability for that example.\n",
        "\n",
        "---\n",
        "\n",
        "### Regularized Objective Function\n",
        "\n",
        "With regularization, the loss function becomes:\n",
        "\n",
        "$$\n",
        "L_{reg}(\\beta) = L(\\beta) + \\lambda R(\\beta)\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $\\lambda \\geq 0$ is the **regularization parameter** (controls strength of regularization),\n",
        "* $R(\\beta)$ is the **regularization term** (penalty on coefficients).\n",
        "\n",
        "The goal is to **minimize $L_{reg}(\\beta)$**, balancing fit to the data with model complexity.\n",
        "\n",
        "---\n",
        "\n",
        "### Types of Regularization\n",
        "\n",
        "#### 1. **L2 Regularization (Ridge Regression)**\n",
        "\n",
        "* Penalizes the **sum of squared coefficients**:\n",
        "\n",
        "$$\n",
        "R(\\beta) = \\frac{1}{2} \\sum_{j=1}^n \\beta_j^2\n",
        "$$\n",
        "\n",
        "* Shrinks coefficients towards zero but usually does not make them exactly zero.\n",
        "* Encourages **small, evenly distributed weights**.\n",
        "* Helps reduce model complexity and **multicollinearity**.\n",
        "\n",
        "#### 2. **L1 Regularization (Lasso Regression)**\n",
        "\n",
        "* Penalizes the **sum of absolute values** of coefficients:\n",
        "\n",
        "$$\n",
        "R(\\beta) = \\sum_{j=1}^n |\\beta_j|\n",
        "$$\n",
        "\n",
        "* Can shrink some coefficients **exactly to zero**, effectively performing **feature selection**.\n",
        "* Leads to sparse models with fewer variables.\n",
        "\n",
        "---\n",
        "\n",
        "### Why is Regularization Needed?\n",
        "\n",
        "#### 1. **Prevent Overfitting**\n",
        "\n",
        "* In high-dimensional data or when there are many features, logistic regression can fit the training data too closely, capturing noise instead of the true pattern.\n",
        "* Regularization limits model complexity by constraining coefficient sizes, reducing variance and improving **generalization** to new data.\n",
        "\n",
        "#### 2. **Handle Multicollinearity**\n",
        "\n",
        "* When features are highly correlated, logistic regression coefficients can become unstable.\n",
        "* L2 regularization stabilizes these coefficients by shrinking them, improving robustness.\n",
        "\n",
        "#### 3. **Improve Model Interpretability**\n",
        "\n",
        "* Especially with L1 regularization, irrelevant or less important features get coefficients shrunk to zero.\n",
        "* This leads to simpler models that are easier to interpret.\n",
        "\n",
        "#### 4. **Control Model Complexity**\n",
        "\n",
        "* Regularization provides a **trade-off** between fitting the training data well and keeping the model simple.\n",
        "* The regularization parameter $\\lambda$ can be tuned (e.g., via cross-validation) to achieve the best balance.\n",
        "\n",
        "---\n",
        "\n",
        "### Intuition\n",
        "\n",
        "* Without regularization, logistic regression can assign very large coefficients to features to perfectly fit training data, but such a model will perform poorly on new data.\n",
        "* Regularization acts like a **“soft constraint”** or **penalty**, discouraging large coefficients and forcing the model to focus on the most important features.\n",
        "* Think of it as adding a cost for complexity — the model tries to minimize errors but also keeps its weights small.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "\n",
        "| Aspect                    | Without Regularization                              | With Regularization                  |\n",
        "| ------------------------- | --------------------------------------------------- | ------------------------------------ |\n",
        "| Risk of Overfitting       | High, especially with many features                 | Reduced, better generalization       |\n",
        "| Model Complexity          | Can be very high                                    | Controlled by shrinking coefficients |\n",
        "| Feature Selection         | No (all features included)                          | Possible with L1 (sparse models)     |\n",
        "| Stability of Coefficients | Can be unstable (especially with multicollinearity) | More stable and robust               |\n",
        "| Interpretability          | Potentially low                                     | Often improved                       |\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Regularization in logistic regression is an essential technique to:\n",
        "\n",
        "* Prevent overfitting,\n",
        "* Improve prediction performance on unseen data,\n",
        "* Handle correlated features,\n",
        "* Enhance model interpretability (especially with L1),\n",
        "* Control the complexity of the model.\n",
        "\n",
        "By adding a penalty term to the loss function, regularization encourages simpler models that better capture the underlying structure of the data, leading to improved reliability and accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "Question 4: What are some common evaluation metrics for classification models, and why are they important?\n",
        "\n",
        "\n",
        "### Introduction\n",
        "\n",
        "In machine learning, especially in classification problems, evaluating how well a model performs is crucial. **Evaluation metrics** provide quantitative measures to assess the effectiveness of a classification model. Choosing appropriate metrics helps us understand the model's strengths, weaknesses, and areas for improvement, ensuring it meets the desired objectives.\n",
        "\n",
        "---\n",
        "\n",
        "### Why Are Evaluation Metrics Important?\n",
        "\n",
        "* **Performance Assessment:** Metrics quantify how accurately a model predicts classes on new, unseen data.\n",
        "* **Model Comparison:** They allow comparing different models or algorithms to select the best one.\n",
        "* **Business Impact:** Metrics help relate model performance to real-world goals (e.g., minimizing false negatives in disease diagnosis).\n",
        "* **Identify Errors:** Metrics highlight specific types of errors (false positives or false negatives), guiding improvements.\n",
        "* **Avoid Misleading Results:** Accuracy alone can be misleading, especially in imbalanced datasets, so multiple metrics provide a more complete picture.\n",
        "\n",
        "---\n",
        "\n",
        "### Common Evaluation Metrics for Classification\n",
        "\n",
        "#### 1. **Accuracy**\n",
        "\n",
        "* **Definition:** The proportion of correctly classified instances (both positives and negatives) out of all instances.\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "$$\n",
        "\n",
        "* Where:\n",
        "\n",
        "  * $TP$: True Positives (correctly predicted positives)\n",
        "  * $TN$: True Negatives (correctly predicted negatives)\n",
        "  * $FP$: False Positives (incorrectly predicted positives)\n",
        "  * $FN$: False Negatives (incorrectly predicted negatives)\n",
        "\n",
        "* **When to use:** Balanced datasets where the classes have roughly equal representation.\n",
        "\n",
        "* **Limitation:** Can be misleading in imbalanced datasets (e.g., if 95% are negative, predicting all negatives yields 95% accuracy but poor performance).\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Precision**\n",
        "\n",
        "* **Definition:** Of all instances predicted positive, how many are actually positive?\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "$$\n",
        "\n",
        "* **Interpretation:** High precision means fewer false positives.\n",
        "* **When to use:** When the cost of false positives is high (e.g., spam filters where wrongly labeling legitimate emails as spam is costly).\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Recall (Sensitivity or True Positive Rate)**\n",
        "\n",
        "* **Definition:** Of all actual positive instances, how many were correctly predicted?\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "$$\n",
        "\n",
        "* **Interpretation:** High recall means fewer false negatives.\n",
        "* **When to use:** When missing positive cases is costly (e.g., disease diagnosis where missing a sick patient is dangerous).\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **F1-Score**\n",
        "\n",
        "* **Definition:** The harmonic mean of Precision and Recall, providing a balance between them.\n",
        "\n",
        "$$\n",
        "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "* **Interpretation:** Useful when you want to balance false positives and false negatives.\n",
        "* **When to use:** When the dataset is imbalanced and you want a single metric capturing both Precision and Recall.\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. **Specificity (True Negative Rate)**\n",
        "\n",
        "* **Definition:** Of all actual negatives, how many were correctly predicted?\n",
        "\n",
        "$$\n",
        "\\text{Specificity} = \\frac{TN}{TN + FP}\n",
        "$$\n",
        "\n",
        "* **Interpretation:** Measures how well the model avoids false positives.\n",
        "* **When to use:** Important in scenarios where false positives have a high cost.\n",
        "\n",
        "---\n",
        "\n",
        "#### 6. **Receiver Operating Characteristic (ROC) Curve and Area Under the Curve (AUC)**\n",
        "\n",
        "* **ROC Curve:** Plots True Positive Rate (Recall) against False Positive Rate $\\left(\\frac{FP}{FP + TN}\\right)$ at different classification thresholds.\n",
        "\n",
        "* **AUC:** The area under the ROC curve, representing the model’s ability to discriminate between classes.\n",
        "\n",
        "* **Interpretation:** AUC close to 1 means excellent discrimination; 0.5 means random guessing.\n",
        "\n",
        "* **When to use:** To evaluate model performance across different thresholds and assess overall classification ability.\n",
        "\n",
        "---\n",
        "\n",
        "#### 7. **Confusion Matrix**\n",
        "\n",
        "* A table showing counts of TP, TN, FP, and FN.\n",
        "\n",
        "|                     | Predicted Positive  | Predicted Negative  |\n",
        "| ------------------- | ------------------- | ------------------- |\n",
        "| **Actual Positive** | True Positive (TP)  | False Negative (FN) |\n",
        "| **Actual Negative** | False Positive (FP) | True Negative (TN)  |\n",
        "\n",
        "* **Use:** Provides a complete snapshot of model errors and correct predictions.\n",
        "* **Helps:** Calculate all other metrics.\n",
        "\n",
        "---\n",
        "\n",
        "### Importance of Using Multiple Metrics\n",
        "\n",
        "* No single metric tells the whole story.\n",
        "* Depending on the context (imbalanced data, cost of errors), different metrics matter.\n",
        "* For example, in medical diagnosis, **Recall** is critical to catch all positives.\n",
        "* In email spam filtering, **Precision** matters to avoid false alarms.\n",
        "* Combining metrics like F1-score and AUC provides a balanced evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary Table\n",
        "\n",
        "| Metric      | Formula                 | Focus                      | Use Case                         |\n",
        "| ----------- | ----------------------- | -------------------------- | -------------------------------- |\n",
        "| Accuracy    | $\\frac{TP + TN}{Total}$ | Overall correctness        | Balanced datasets                |\n",
        "| Precision   | $\\frac{TP}{TP + FP}$    | Minimizing false positives | Spam detection                   |\n",
        "| Recall      | $\\frac{TP}{TP + FN}$    | Minimizing false negatives | Disease diagnosis                |\n",
        "| F1-Score    | Harmonic mean of P & R  | Balance precision & recall | Imbalanced datasets              |\n",
        "| Specificity | $\\frac{TN}{TN + FP}$    | Minimizing false positives | When false positives costly      |\n",
        "| AUC-ROC     | Area under ROC curve    | Overall discrimination     | Threshold-independent evaluation |\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Evaluation metrics are fundamental tools to understand how well a classification model performs. They help in:\n",
        "\n",
        "* Quantifying the accuracy and types of errors,\n",
        "* Guiding model selection and tuning,\n",
        "* Aligning model performance with real-world priorities,\n",
        "* Avoiding pitfalls like misleading high accuracy on imbalanced datasets.\n",
        "\n",
        "Choosing the right metrics ensures building reliable and effective classification models tailored to specific problem needs.\n",
        "\n",
        "---\n",
        "\n",
        "Question 5: Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
        "splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Display first few rows of the DataFrame (optional)\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Split dataset into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)  # Increased max_iter to ensure convergence\n",
        "\n",
        "# Train the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print accuracy\n",
        "print(f\"\\nAccuracy of Logistic Regression model on test set: {accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Sample Output\n",
        "\n",
        "```\n",
        "First 5 rows of the dataset:\n",
        "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  ...  worst fractal dimension  target\n",
        "0        17.99         10.38          122.80     1001.0           0.1184  ...                 0.11890       0\n",
        "1        20.57         17.77          132.90     1326.0           0.0847  ...                 0.08902       0\n",
        "2        19.69         21.25          130.00     1203.0           0.1096  ...                 0.08758       0\n",
        "3        11.42         20.38           77.58      386.1           0.1425  ...                 0.17300       0\n",
        "4        20.29         14.34          135.10     1297.0           0.1003  ...                 0.07678       0\n",
        "\n",
        "Accuracy of Logistic Regression model on test set: 0.9737\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "Question 6:  Write a Python program to train a Logistic Regression model using L2\n",
        "regularization (Ridge) and print the model coefficients and accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Split dataset into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model with L2 regularization (default)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=10000)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print model coefficients\n",
        "print(\"Model Coefficients (weights) for each feature:\")\n",
        "for feature, coef in zip(data.feature_names, model.coef_[0]):\n",
        "    print(f\"{feature}: {coef:.4f}\")\n",
        "\n",
        "# Print accuracy\n",
        "print(f\"\\nAccuracy of Logistic Regression model with L2 regularization on test set: {accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Sample Output\n",
        "\n",
        "```\n",
        "Model Coefficients (weights) for each feature:\n",
        "mean radius: -0.0524\n",
        "mean texture: 0.0530\n",
        "mean perimeter: 0.1019\n",
        "mean area: 0.0008\n",
        "mean smoothness: 0.5942\n",
        "mean compactness: 0.0899\n",
        "mean concavity: -0.0927\n",
        "mean concave points: 0.4134\n",
        "mean symmetry: -0.1901\n",
        "mean fractal dimension: -0.1036\n",
        "radius error: 0.0533\n",
        "texture error: -0.0264\n",
        "perimeter error: -0.0171\n",
        "area error: -0.0013\n",
        "smoothness error: 0.1550\n",
        "compactness error: 0.0533\n",
        "concavity error: -0.1405\n",
        "concave points error: -0.0412\n",
        "symmetry error: -0.0065\n",
        "fractal dimension error: 0.0184\n",
        "worst radius: 0.0121\n",
        "worst texture: 0.0140\n",
        "worst perimeter: -0.0222\n",
        "worst area: -0.0019\n",
        "worst smoothness: 0.3179\n",
        "worst compactness: 0.1425\n",
        "worst concavity: -0.0949\n",
        "worst concave points: 0.1714\n",
        "worst symmetry: 0.0730\n",
        "worst fractal dimension: -0.0163\n",
        "\n",
        "Accuracy of Logistic Regression model with L2 regularization on test set: 0.9737\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "Question 7: Write a Python program to train a Logistic Regression model for multiclass\n",
        "classification using multi_class='ovr' and print the classification report.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Split dataset into features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model for multiclass classification using One-vs-Rest ('ovr')\n",
        "model = LogisticRegression(multi_class='ovr', max_iter=10000, solver='lbfgs')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report for Logistic Regression (One-vs-Rest):\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Sample Output\n",
        "\n",
        "```\n",
        "Classification Report for Logistic Regression (One-vs-Rest):\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "      setosa       1.00      1.00      1.00        10\n",
        "  versicolor       1.00      0.92      0.96        13\n",
        "   virginica       0.91      1.00      0.95         7\n",
        "\n",
        "    accuracy                           0.97        30\n",
        "   macro avg       0.97      0.97      0.97        30\n",
        "weighted avg       0.98      0.97      0.97        30\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "Question 8: Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "hyperparameters for Logistic Regression and print the best parameters and validation\n",
        "accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "\n",
        "```python\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Prepare DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Split into features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000, solver='liblinear')  # solver='liblinear' supports both l1 and l2 penalties\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],   # Regularization strength\n",
        "    'penalty': ['l1', 'l2']          # Regularization types\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Predict on test data using the best estimator\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate test accuracy\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(\"Best hyperparameters found by GridSearchCV:\")\n",
        "print(best_params)\n",
        "print(f\"\\nValidation accuracy with best parameters on test set: {test_accuracy:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Sample Output\n",
        "\n",
        "```\n",
        "Best hyperparameters found by GridSearchCV:\n",
        "{'C': 1, 'penalty': 'l2'}\n",
        "\n",
        "Validation accuracy with best parameters on test set: 0.9737\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "Question 9: Write a Python program to standardize the features before training Logistic\n",
        "Regression and compare the model's accuracy with and without scaling.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# -------- Without Scaling --------\n",
        "# Create and train Logistic Regression model without scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=10000, random_state=42)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# -------- With Scaling --------\n",
        "# Initialize StandardScaler and scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train Logistic Regression model with scaling\n",
        "model_with_scaling = LogisticRegression(max_iter=10000, random_state=42)\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict and calculate accuracy\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy without feature scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with feature scaling:    {accuracy_with_scaling:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Explanation:\n",
        "\n",
        "* We use the **Breast Cancer** dataset from `sklearn.datasets`.\n",
        "* We split data into training (80%) and testing (20%).\n",
        "* First, the Logistic Regression model is trained **without any scaling**.\n",
        "* Then, we standardize features using `StandardScaler` (mean=0, variance=1).\n",
        "* Train Logistic Regression again with the scaled data.\n",
        "* Finally, print and compare the accuracy of both models.\n",
        "\n",
        "---\n",
        "\n",
        "### Sample Output:\n",
        "\n",
        "```\n",
        "Accuracy without feature scaling: 0.9123\n",
        "Accuracy with feature scaling:    0.9474\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "You can see that **scaling the features improved the accuracy** of the Logistic Regression model on this dataset.\n",
        "\n",
        "---\n",
        "\n",
        "Question 10: Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced\n",
        "dataset (only 5% of customers respond), describe the approach you’d take to build a\n",
        "Logistic Regression model — including data handling, feature scaling, balancing\n",
        "classes, hyperparameter tuning, and evaluating the model for this real-world business\n",
        "use case.\n",
        "\n",
        "\n",
        "### Answer:\n",
        "\n",
        "When working with an imbalanced dataset (where only 5% of customers respond to a marketing campaign), building a robust Logistic Regression model requires careful attention to data preprocessing, handling imbalance, and evaluation to ensure the model performs well on minority class prediction.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Data Handling and Preprocessing**\n",
        "\n",
        "* **Data Cleaning:**\n",
        "\n",
        "  * Handle missing values (imputation or removal).\n",
        "  * Remove duplicate entries.\n",
        "  * Encode categorical variables (using one-hot encoding or label encoding).\n",
        "  * Feature engineering: create relevant features that could help in prediction.\n",
        "\n",
        "* **Train-Test Split:**\n",
        "\n",
        "  * Split the dataset into training and test sets (e.g., 80:20) using stratified sampling to maintain the 5% response rate distribution in both sets.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Feature Scaling**\n",
        "\n",
        "* Logistic Regression is sensitive to feature scales.\n",
        "* Apply **StandardScaler** or **MinMaxScaler** to scale numerical features to a common scale (mean=0 and variance=1).\n",
        "* Important to fit the scaler only on training data and transform both training and test data accordingly.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Handling Imbalanced Classes**\n",
        "\n",
        "Since only 5% of customers respond, the dataset is highly imbalanced, which may cause the model to be biased toward the majority class (non-responders).\n",
        "\n",
        "Several strategies can be applied:\n",
        "\n",
        "* **Resampling Techniques:**\n",
        "\n",
        "  * **Oversampling:** Use techniques like **SMOTE (Synthetic Minority Over-sampling Technique)** to synthetically increase minority class samples in the training data.\n",
        "\n",
        "  * **Undersampling:** Randomly reduce majority class samples to balance the classes.\n",
        "\n",
        "  * Sometimes a combination of both (hybrid methods) work well.\n",
        "\n",
        "* **Class Weighting:**\n",
        "\n",
        "  * Logistic Regression supports `class_weight='balanced'` to automatically adjust weights inversely proportional to class frequencies, penalizing misclassification of minority class more heavily.\n",
        "\n",
        "* **Choose one or combine methods** based on validation performance.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Building the Logistic Regression Model**\n",
        "\n",
        "* Use **regularized logistic regression** (e.g., L2 regularization) to avoid overfitting.\n",
        "\n",
        "* Pass `class_weight='balanced'` as a parameter or use resampled data to handle class imbalance.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Hyperparameter Tuning**\n",
        "\n",
        "* Perform hyperparameter tuning with **Grid Search** or **Randomized Search** cross-validation to find the best parameters:\n",
        "\n",
        "  * **Regularization strength (`C`)**: controls the inverse of regularization strength. Smaller values specify stronger regularization.\n",
        "\n",
        "  * **Penalty type**: L1 (lasso) or L2 (ridge) regularization.\n",
        "\n",
        "  * **Solver**: e.g., `liblinear` supports both L1 and L2; `lbfgs` for L2.\n",
        "\n",
        "* Use **Stratified K-Fold Cross Validation** to maintain class distribution during training and validation.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **Model Evaluation**\n",
        "\n",
        "Since the dataset is imbalanced, **accuracy is not a good metric** because predicting all non-responders would still yield 95% accuracy but no business value.\n",
        "\n",
        "Preferred evaluation metrics include:\n",
        "\n",
        "* **Precision:** Proportion of predicted responders that are actual responders (reduces false positives).\n",
        "\n",
        "* **Recall (Sensitivity):** Proportion of actual responders correctly predicted (important to identify responders).\n",
        "\n",
        "* **F1-Score:** Harmonic mean of precision and recall, balances both.\n",
        "\n",
        "* **ROC-AUC:** Area under the receiver operating characteristic curve, measures the trade-off between true positive rate and false positive rate.\n",
        "\n",
        "* **PR-AUC:** Area under the precision-recall curve, especially useful in highly imbalanced datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **Business Considerations**\n",
        "\n",
        "* **Cost-Sensitive Analysis:**\n",
        "  Evaluate the cost of false negatives (missing responders) versus false positives (targeting non-responders). Adjust threshold accordingly.\n",
        "\n",
        "* **Threshold Tuning:**\n",
        "  Instead of the default 0.5 probability cutoff, adjust the decision threshold to optimize business metrics such as maximizing ROI or minimizing marketing costs.\n",
        "\n",
        "* **Interpretability:**\n",
        "  Logistic Regression coefficients can provide insights into which features drive customer response, helping business stakeholders understand and trust the model.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. **Summary of Workflow**\n",
        "\n",
        "| Step                  | Description                                      |\n",
        "| --------------------- | ------------------------------------------------ |\n",
        "| Data Cleaning         | Handle missing data, encoding categorical vars   |\n",
        "| Train-Test Split      | Stratified split to maintain class balance       |\n",
        "| Feature Scaling       | Scale numerical features with StandardScaler     |\n",
        "| Imbalance Handling    | Use SMOTE/undersampling or class weights         |\n",
        "| Model Building        | Train Logistic Regression with regularization    |\n",
        "| Hyperparameter Tuning | Grid search with stratified CV for C and penalty |\n",
        "| Model Evaluation      | Use precision, recall, F1-score, ROC-AUC         |\n",
        "| Threshold Tuning      | Adjust probability cutoff for business needs     |\n",
        "| Interpretation        | Analyze coefficients for business insights       |\n",
        "\n",
        "---\n",
        "\n",
        "### Final Note:\n",
        "\n",
        "This approach ensures that the Logistic Regression model not only achieves good predictive performance on an imbalanced dataset but also aligns with business objectives by focusing on meaningful evaluation metrics and practical deployment considerations.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ljebXqgT-51u"
      }
    }
  ]
}